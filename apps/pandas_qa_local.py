# coding=utf-8
# Copyright 2018-2023 EvaDB
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os

from gpt4all import GPT4All
import shutil
import subprocess
from typing import Dict


import pandas as pd

APP_SOURCE_DIR = os.path.abspath(os.path.dirname(__file__))
CURRENT_WORKING_DIR = os.getcwd()  # used to locate evadb_data dir

# default file paths
DEFAULT_TEXT_FILE_PATH = os.path.join(APP_SOURCE_DIR, "data", "eva_paper.txt")
MAX_CHUNK_SIZE = 2000

def receive_user_input() -> Dict:
    """Receives user input.

    Returns:
        user_input (dict): global configurations
    """
    print(
        "ðŸ”® Welcome to EvaDB! This app lets you to run data analytics on a csv file like in a conversational manner.\nYou will only need to supply a path to csv file and an OpenAI API key.\n\n"
    )
    user_input = dict()

    text_file_path = str(
        input("ðŸ“‹ Enter the text file path (press Enter to use our default text file): ")
    )

    if text_file_path == "":
        text_file_path = DEFAULT_TEXT_FILE_PATH
    user_input["text_file_path"] = text_file_path

    return user_input

def generate_script(df: pd.DataFrame, question: str) -> str:
    """Generates script with llm.

    Args:
        question (str): question to ask to llm.

    Returns
        str: script generated by llm.
    """
    # generate summary
    all_columns = list(df)  # Creates list of all column headers
    df[all_columns] = df[all_columns].astype(str)

    prompt = f"""There is a dataframe in pandas (python). The name of the
            dataframe is df. This is the result of print(df.head()):
            {str(df.head())}\nAssuming the dataframe is already loaded and named 'df'. Do not include pd.read_csv, do not write code to load the CSV file. Return a python script to get the answer to a question.    
            Question : {question}. """

    llm = GPT4All("llama-2-7b-chat.ggmlv3.q4_0.bin")

    script_body = llm.generate(prompt)
    script_body = script_body.split("```")[1].lstrip("python")
    return script_body



def cleanup():
    """Removes any temporary file / directory created by EvaDB."""
    if os.path.exists("evadb_data"):
        shutil.rmtree("evadb_data")

def split_text_into_chunkss(text, max_chunk_size=MAX_CHUNK_SIZE):
    chunks = []
    current_chunk = ""

    for line in text.splitlines():
        if len(current_chunk) + len(line) + 1 <= max_chunk_size:
            # Add line to the current chunk
            if current_chunk:
                current_chunk += '\n'
            current_chunk += line
        else:
            # Start a new chunk
            chunks.append(current_chunk)
            current_chunk = line

    if current_chunk:
        chunks.append(current_chunk)

    return chunks

def split_text_into_chunks(text, max_chunk_size=MAX_CHUNK_SIZE):
    chunks = []
    start = 0
    end = max_chunk_size

    while start < len(text):
        chunk = text[start:end]
        chunks.append(chunk)
        start = end
        end += max_chunk_size

    return chunks

if __name__ == "__main__":
    # try:
        # receive input from user
        user_input = receive_user_input()
        df = pd.read_csv(user_input["text_file_path"], names=['text'])
        with open("/home/preethi/projects/pandas-ai-integration/apps/data/eva_paper.txt", 'r') as file:
            file_contents = file.read()

        # Split the contents into chunks
        text_chunks = split_text_into_chunks(file_contents)
        chunked_output_file = "data_chunks.txt"
        i = 0
        with open(chunked_output_file, 'w') as chunked_file:
            for chunk in text_chunks:
                i = i + 1
                chunk = chunk + '\n\n\n\n\n\n\n'
                chunked_file.write(chunk)
                print("chunk " + str(i))

        print(f"Text chunks saved to {chunked_output_file}")
        print("here1")
        llm = GPT4All("llama-2-7b-chat.ggmlv3.q4_0.bin")
        summaries = []
        for chunk in text_chunks:
            summaries.append(llm.generate("Summarize this text" + chunk))
        print("SUMMARRYYYYYY", summaries)
        